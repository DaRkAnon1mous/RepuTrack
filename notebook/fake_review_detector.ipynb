{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da2edc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809aa9fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8222a8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc2688bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             category  rating label  \\\n",
       "0  Home_and_Kitchen_5     5.0    CG   \n",
       "1  Home_and_Kitchen_5     5.0    CG   \n",
       "2  Home_and_Kitchen_5     5.0    CG   \n",
       "3  Home_and_Kitchen_5     1.0    CG   \n",
       "4  Home_and_Kitchen_5     5.0    CG   \n",
       "\n",
       "                                               text_  \n",
       "0  Love this!  Well made, sturdy, and very comfor...  \n",
       "1  love it, a great upgrade from the original.  I...  \n",
       "2  This pillow saved my back. I love the look and...  \n",
       "3  Missing information on how to use it, but it i...  \n",
       "4  Very nice set. Good quality. We have had the s...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/fake reviews dataset.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b0fe164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (40432, 4)\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "CG    20216\n",
      "OR    20216\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b995f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if col == \"text_\":\n",
    "        df = df.rename(columns={'text_':\"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34f48801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             category  rating label  \\\n",
       "0  Home_and_Kitchen_5     5.0    CG   \n",
       "1  Home_and_Kitchen_5     5.0    CG   \n",
       "2  Home_and_Kitchen_5     5.0    CG   \n",
       "3  Home_and_Kitchen_5     1.0    CG   \n",
       "4  Home_and_Kitchen_5     5.0    CG   \n",
       "\n",
       "                                                text  \n",
       "0  Love this!  Well made, sturdy, and very comfor...  \n",
       "1  love it, a great upgrade from the original.  I...  \n",
       "2  This pillow saved my back. I love the look and...  \n",
       "3  Missing information on how to use it, but it i...  \n",
       "4  Very nice set. Good quality. We have had the s...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7c7d75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text', 'label']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57a8d539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower()) #- any character that is NOT a letter (a‚Äìz, A‚ÄìZ) or whitespace. - Those characters are replaced with '' \n",
    "    text = re.sub(r'\\s+', ' ', text) #- one or more whitespace characters.- Replaces them with a single space\n",
    "    return text.strip()\n",
    "\n",
    "df['cleaned'] = df['text'].apply(clean_text) #- Applies your clean_text function to each row.\n",
    "df = df[df['cleaned'].str.len() > 10] #removing trivial, noisy, or uninformative samples that could hurt model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ff8e4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "      <td>CG</td>\n",
       "      <td>love this well made sturdy and very comfortabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "      <td>CG</td>\n",
       "      <td>love it a great upgrade from the original ive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "      <td>CG</td>\n",
       "      <td>this pillow saved my back i love the look and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "      <td>CG</td>\n",
       "      <td>missing information on how to use it but it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "      <td>CG</td>\n",
       "      <td>very nice set good quality we have had the set...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label  \\\n",
       "0  Love this!  Well made, sturdy, and very comfor...    CG   \n",
       "1  love it, a great upgrade from the original.  I...    CG   \n",
       "2  This pillow saved my back. I love the look and...    CG   \n",
       "3  Missing information on how to use it, but it i...    CG   \n",
       "4  Very nice set. Good quality. We have had the s...    CG   \n",
       "\n",
       "                                             cleaned  \n",
       "0  love this well made sturdy and very comfortabl...  \n",
       "1  love it a great upgrade from the original ive ...  \n",
       "2  this pillow saved my back i love the look and ...  \n",
       "3  missing information on how to use it but it is...  \n",
       "4  very nice set good quality we have had the set...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b783f98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40430, 3)\n"
     ]
    }
   ],
   "source": [
    "df['label'] = df['label'].map({'CG': 1, 'OR':0})\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0287e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40430,)\n",
      "(40430,)\n"
     ]
    }
   ],
   "source": [
    "X = df['cleaned']\n",
    "y = df['label']\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc05a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 28301, Val: 6064, Test: 6065\n"
     ]
    }
   ],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X,y,test_size=0.3,random_state=42, stratify=y)\n",
    "\n",
    "X_val,X_test,y_val,y_test = train_test_split(X_temp,y_temp,test_size=0.5,random_state=42,stratify=y_temp)\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8899ac9d",
   "metadata": {},
   "source": [
    "DistilBERT is a distilled version of BERT: it keeps the same Transformer encoder ideas but with fewer layers so it runs faster and uses less memory. It learns language patterns from large text corpora and can be fine‚Äëtuned for tasks like classification. The Hugging Face DistilBertForSequenceClassification wraps the encoder and adds a small classification head on top to output logits for each class.\n",
    "\n",
    "Architecture in simple terms\n",
    "- Transformer encoder: reads the whole sentence and builds contextual word vectors (each token‚Äôs meaning depends on the whole sentence).\n",
    "- Distillation: a teacher model (BERT) teaches a smaller student (DistilBERT) to mimic its behavior so the student is compact but still strong.\n",
    "- Classification head: a linear layer on top of the pooled output that maps to num_labels (here 2).\n",
    "\n",
    "\n",
    "üîπ What is PyTorch actually doing?\n",
    "- PyTorch is a deep learning framework. Think of it as the ‚Äúengine‚Äù that lets us build, train, and run neural networks.\n",
    "- It provides:\n",
    "- Tensors (its basic data structure, like NumPy arrays but with GPU support).\n",
    "- Automatic differentiation (so gradients for backpropagation are calculated automatically).\n",
    "- Modules (ready‚Äëmade building blocks like layers, optimizers, losses).\n",
    "- In your code, PyTorch is handling:\n",
    "- The dataset (Dataset and DataLoader classes).\n",
    "- The training loop (forward pass, loss calculation, backward pass, optimizer step).\n",
    "- Moving data and models to GPU/CPU (.to(device)).\n",
    "\n",
    "üîπ What are Tensors?\n",
    "- A tensor is just a multi‚Äëdimensional array (like a matrix, but more general).\n",
    "- Example:\n",
    "- Scalar ‚Üí 5 (0D tensor).\n",
    "- Vector ‚Üí [1,2,3] (1D tensor).\n",
    "- Matrix ‚Üí [[1,2],[3,4]] (2D tensor).\n",
    "- Higher dimensions ‚Üí images, batches of text, etc.\n",
    "- PyTorch tensors can live on GPU, which makes training super fast compared to normal Python arrays.\n",
    "\n",
    "üîπ What is the Attention Mask?\n",
    "- When we tokenize text, we pad shorter sentences to a fixed length (say 256 tokens).\n",
    "- Example: \"hello world\" ‚Üí [101, 7592, 2088, 102, 0, 0, 0...] (zeros are padding).\n",
    "- The attention mask tells the model which tokens are real and which are just padding.\n",
    "- 1 ‚Üí real token.\n",
    "- 0 ‚Üí padding.\n",
    "- Without this, the model would waste attention on meaningless padding tokens.\n",
    "\n",
    " Why do we need tensors?\n",
    "- Deep learning models (like DistilBERT) are basically giant math machines.\n",
    "- They don‚Äôt understand text directly ‚Äî they understand numbers.\n",
    "- A tensor is the data structure PyTorch uses to hold these numbers.\n",
    "- Think of a tensor as a container for numbers that can be:\n",
    "- 1D (like a list of token IDs for one sentence).\n",
    "- 2D (like a batch of sentences, each with token IDs).\n",
    "- 3D+ (images, video frames, etc.).\n",
    "The key advantage: tensors can live on the GPU, so millions of operations can be done in parallel very fast. Without tensors, training would be painfully slow.\n",
    "\n",
    "üîπ What help are tensors doing?\n",
    "- They allow PyTorch to:\n",
    "- Store the tokenized text (numbers instead of words).\n",
    "- Store the attention mask (1s and 0s for real vs. padded tokens).\n",
    "- Store the labels (like 0 for \"Fake\", 1 for \"Real\").\n",
    "- During training:\n",
    "- These tensors are fed into the model.\n",
    "- The model does matrix multiplications and attention calculations on them.\n",
    "- Gradients (also tensors) are computed and used to update weights.\n",
    "So tensors are the bridge between your text data and the math inside the neural network.\n",
    "\n",
    "üîπ Why is the dataset being sent as a list?\n",
    "When you create ReviewDataset(X_train.tolist(), y_train.tolist(), tokenizer):\n",
    "- X_train and y_train are often NumPy arrays or Pandas series.\n",
    "- .tolist() converts them into Python lists because the custom Dataset class expects normal lists it can index with __getitem__.\n",
    "- Lists are easier for PyTorch‚Äôs Dataset to handle when fetching samples one by one.\n",
    "So: lists = simple containers of text and labels ‚Üí dataset wraps them ‚Üí DataLoader batches them.\n",
    "\n",
    "üîπ What is the DataLoader and what is it doing?\n",
    "Think of DataLoader as a waiter in a restaurant:\n",
    "- The Dataset is the kitchen (it has all the food = samples).\n",
    "- The DataLoader is the waiter who brings food to the table in small batches.\n",
    "- The Model is the customer who eats the food (learns from the data).\n",
    "What DataLoader does:\n",
    "- Batching: Instead of giving the model one sentence at a time, it gives (say) 16 sentences together. This speeds up training and stabilizes learning.\n",
    "- Shuffling: Randomizes the order of samples each epoch so the model doesn‚Äôt memorize sequence patterns.\n",
    "- Parallel loading: Can fetch data using multiple workers (threads) to keep GPU busy.\n",
    "So DataLoader = efficient delivery system for data ‚Üí model.\n",
    "\n",
    "üîπ What is AdamW and how does it optimize the model?\n",
    "Optimization = how the model learns by adjusting its weights.\n",
    "AdamW in simple terms:\n",
    "- Adam = Adaptive Moment Estimation. It‚Äôs like a smart gradient descent:\n",
    "- It looks at the gradient (direction of error).\n",
    "- It keeps track of past gradients (momentum) so it doesn‚Äôt zig‚Äëzag too much.\n",
    "- It adapts the learning rate for each parameter individually.\n",
    "- W = Weight Decay. This prevents weights from growing too large (regularization).\n",
    "What happens during optimization:\n",
    "- Model makes a prediction ‚Üí compares with true label ‚Üí computes loss.\n",
    "- PyTorch calculates gradients (how much each weight contributed to the error).\n",
    "- AdamW uses these gradients to adjust weights slightly in the right direction.\n",
    "- Example: if a weight made the model predict too high, AdamW nudges it lower.\n",
    "- Repeat for many batches ‚Üí model gradually learns patterns in the data.\n",
    "\n",
    "üîπ Putting it all together (flow)\n",
    "- Dataset ‚Üí holds text + labels.\n",
    "- DataLoader ‚Üí batches them into tensors.\n",
    "- Model ‚Üí takes tensors, runs forward pass, outputs predictions.\n",
    "- Loss ‚Üí measures how wrong predictions are.\n",
    "- Backpropagation ‚Üí computes gradients.\n",
    "- AdamW optimizer ‚Üí updates weights using gradients.\n",
    "- Repeat for many epochs ‚Üí model improves accuracy.\n",
    "\n",
    "‚ö° Simple analogy:\n",
    "- Dataset = library of books.\n",
    "- DataLoader = librarian who brings 16 books at a time.\n",
    "- Model = student reading books.\n",
    "- Loss = exam score showing mistakes.\n",
    "- AdamW = teacher correcting the student‚Äôs notes so they improve next time.\n",
    "\n",
    "\n",
    "## üîπ Why do we need the loop for each epoch?\n",
    "Let‚Äôs break it down:\n",
    "\n",
    "### 1. `model.train()` alone is not enough\n",
    "- `model.train()` just **switches the model into training mode** (turns on dropout, etc.).  \n",
    "- It does **not** actually train the model.  \n",
    "- Training requires:  \n",
    "  - Feeding data (forward pass).  \n",
    "  - Calculating loss.  \n",
    "  - Backpropagation (gradients).  \n",
    "  - Optimizer step (update weights).  \n",
    "- That‚Äôs why we need the loop ‚Äî to actually perform these steps repeatedly.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. The **epoch loop** ensures repeated exposure\n",
    "- One epoch = model sees the entire dataset once.  \n",
    "- But one pass is not enough ‚Äî the model won‚Äôt learn well from just one look.  \n",
    "- Multiple epochs = repeated practice.  \n",
    "- Each epoch refines the weights further, reducing loss and improving accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. The **batch loop** inside each epoch\n",
    "- Datasets are too big to feed all at once (memory issue).  \n",
    "- So we split into **mini‚Äëbatches** (e.g., 16 samples).  \n",
    "- For each batch:\n",
    "  1. Forward pass ‚Üí model predicts.  \n",
    "  2. Loss ‚Üí compare prediction vs. true labels.  \n",
    "  3. Backward pass ‚Üí compute gradients.  \n",
    "  4. Optimizer step ‚Üí update weights.  \n",
    "- Repeat until all batches are done ‚Üí that completes one epoch.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Why can‚Äôt we just do one epoch?\n",
    "- Imagine learning multiplication tables:\n",
    "  - If you study them once, you‚Äôll forget quickly.  \n",
    "  - If you repeat them multiple times, you get better and faster.  \n",
    "- Same with the model:  \n",
    "  - One epoch = rough first attempt.  \n",
    "  - More epochs = gradual refinement.  \n",
    "  - Too many epochs = risk of memorizing (overfitting).  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Full training loop explained in simple flow\n",
    "1. **Set training mode** ‚Üí `model.train()`.  \n",
    "2. **Epoch loop** ‚Üí repeat training passes (e.g., 3 times).  \n",
    "3. **Batch loop** ‚Üí for each mini‚Äëbatch:  \n",
    "   - Get tensors (`input_ids`, `attention_mask`, `labels`).  \n",
    "   - Forward pass ‚Üí predictions.  \n",
    "   - Loss ‚Üí how wrong predictions are.  \n",
    "   - Backward pass ‚Üí compute gradients.  \n",
    "   - Optimizer step ‚Üí update weights.  \n",
    "4. **Print loss** ‚Üí see how much error remains after each epoch.  \n",
    "\n",
    "---\n",
    "\n",
    "‚ö° **Analogy:**  \n",
    "Think of training like practicing basketball free throws:  \n",
    "- **Batch loop** = each shot you take.  \n",
    "- **Epoch loop** = one full practice session (all shots).  \n",
    "- **Multiple epochs** = multiple practice sessions over days.  \n",
    "- Just saying ‚ÄúI‚Äôm practicing‚Äù (`model.train()`) doesn‚Äôt make you better ‚Äî you actually need to take the shots (the loops).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d81915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset): #defines a PyTorch dataset wrapperdefines a PyTorch dataset wrapper\n",
    "    def __init__(self,texts,labels,tokenizer,max_len=256): #- stores texts, labels, the tokenizer, and a max length for padding/truncation.\n",
    "        self.texts = texts \n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer \n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self): # tells PyTorch how many samples exist.\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self ,idx):\n",
    "        text = str(self.texts[idx]) #fetch raw text and label. #- idx is the index of the sample to retrieve.\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer( #- tokenizer converts text to input_ids and attention_mask tensors; truncation and padding='max_length' ensure fixed length\n",
    "            text,\n",
    "            truncation = True,\n",
    "            padding='max_length',# Cut off if longer than max_len\n",
    "            max_length=self.max_len,\n",
    "            return_tensors ='pt'# Return PyTorch tensors\n",
    "        )\n",
    "        return{\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9cf1eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\Coding Workspaces\\RepuTrack\\.venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased') #downloads tokenizer vocab and rules\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2) #downloads pre-trained DistilBERT model with a classification head for 2 classes\n",
    "model.to(device) # moves model to GPU/CPU\n",
    "\n",
    "#Dataset \n",
    "train_dataset = ReviewDataset(X_train.tolist(), y_train.tolist(), tokenizer)\n",
    "val_dataset = ReviewDataset(X_val.tolist(), y_val.tolist(), tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset , batch_size = 16, shuffle= True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b57cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DistilBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1769/1769 [3:32:42<00:00,  7.21s/it]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.1495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1769/1769 [3:04:04<00:00,  6.24s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 0.0510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1769/1769 [2:27:07<00:00,  4.99s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Loss: 0.0251\n",
      "DistilBERT Training Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer model    \n",
    "\n",
    "model.train()\n",
    "\n",
    "print(\"Training DistilBERT...\")\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader): # - tqdm is just a progress bar library in Python.\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"DistilBERT Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c0e4a",
   "metadata": {},
   "source": [
    "\n",
    "## üîπ First, the big picture\n",
    "In **machine learning (ML)** you usually do:\n",
    "- Train on **X_train, y_train**  \n",
    "- Test on **X_test, y_test**  \n",
    "- Compare predictions (`y_pred`) with true labels (`y_test`)  \n",
    "\n",
    "In **PyTorch NLP**, the idea is the same ‚Äî but instead of plain arrays, we use **tensors + DataLoader** to feed the model.  \n",
    "\n",
    "So evaluation = **feed unseen data into the trained model ‚Üí collect predictions ‚Üí compare with true labels ‚Üí compute accuracy/metrics.**\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Your evaluation code\n",
    "```python\n",
    "model.eval()\n",
    "preds = []\n",
    "true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        batch_preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "        preds.extend(batch_preds)\n",
    "        true.extend(batch['labels'].numpy())\n",
    "\n",
    "print(\"DistilBERT Accuracy:\", accuracy_score(true, preds))\n",
    "print(classification_report(true, preds, target_names=['Real', 'Fake']))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step by step explanation\n",
    "\n",
    "### 1. `model.eval()`\n",
    "- Switches the model into **evaluation mode**.  \n",
    "- This turns off things like **dropout** (randomly dropping neurons during training).  \n",
    "- Ensures predictions are stable and consistent.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `with torch.no_grad():`\n",
    "- Tells PyTorch: ‚ÄúDon‚Äôt calculate gradients now.‚Äù  \n",
    "- Why? Because during evaluation we don‚Äôt need backpropagation ‚Äî we‚Äôre just checking performance.  \n",
    "- Saves memory and speeds things up.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Looping through `val_loader`\n",
    "- `val_loader` is the DataLoader for your **validation set** (X_val, y_val).  \n",
    "- It gives batches of tokenized text + labels.  \n",
    "- For each batch:\n",
    "  - `input_ids` ‚Üí tokenized text.  \n",
    "  - `attention_mask` ‚Üí tells model which tokens are padding.  \n",
    "  - `labels` ‚Üí true class (0 or 1).  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Forward pass (getting predictions)\n",
    "```python\n",
    "outputs = model(input_ids, attention_mask=attention_mask)\n",
    "```\n",
    "- The model processes the batch and outputs **logits** (raw scores for each class).  \n",
    "- Example: for binary classification, logits might look like `[2.3, -1.1]` ‚Üí meaning class 0 is more likely.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Converting logits to predictions\n",
    "```python\n",
    "batch_preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "```\n",
    "- `torch.argmax(..., dim=1)` ‚Üí picks the class with the highest score.  \n",
    "- Converts tensor ‚Üí NumPy array ‚Üí easy to store.  \n",
    "- Example: `[2.3, -1.1]` ‚Üí prediction = `0`.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Collecting predictions and true labels\n",
    "```python\n",
    "preds.extend(batch_preds)\n",
    "true.extend(batch['labels'].numpy())\n",
    "```\n",
    "- `preds` = all predicted labels across batches.  \n",
    "- `true` = all actual labels across batches.  \n",
    "- At the end, you have two lists: just like `y_pred` and `y_test` in ML.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Computing metrics\n",
    "```python\n",
    "print(\"DistilBERT Accuracy:\", accuracy_score(true, preds))\n",
    "print(classification_report(true, preds, target_names=['Real', 'Fake']))\n",
    "```\n",
    "- **Accuracy** = fraction of correct predictions.  \n",
    "- **Classification report** = precision, recall, F1‚Äëscore for each class (`Real`, `Fake`).  \n",
    "- This is exactly like scikit‚Äëlearn evaluation, just with tensors ‚Üí arrays conversion.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Why 3 splits (train, val, test)?\n",
    "- **Train set (X_train, y_train)** ‚Üí used to teach the model (update weights).  \n",
    "- **Validation set (X_val, y_val)** ‚Üí used during training to check performance and tune hyperparameters (like learning rate, batch size).  \n",
    "- **Test set (X_test, y_test)** ‚Üí final unseen data to measure how well the model generalizes.  \n",
    "\n",
    "üëâ In your code, they used **train + val**. Often after training, you‚Äôd also evaluate on **test** to report final accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "‚ö° **Analogy:**  \n",
    "- **Train set** = practice questions.  \n",
    "- **Validation set** = mock exam to check progress.  \n",
    "- **Test set** = real exam to measure final ability.  \n",
    "- Evaluation loop = grading the exam: collect answers (preds), compare with correct answers (true), compute score (accuracy/F1).\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ So evaluation in PyTorch NLP is the same idea as ML:  \n",
    "- Get predictions ‚Üí compare with true labels ‚Üí compute metrics.  \n",
    "- The only difference is the **DataLoader + tensors** machinery feeding the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9d51211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# distilbert_tokenizer = AutoTokenizer.from_pretrained(\"../backend/nlp/distilbert_fake_review\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"../backend/nlp/distilbert_fake_review\")\n",
    "# distilbert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b63e7914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT Accuracy: 0.9736147757255936\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.98      0.96      0.97      3032\n",
      "        Fake       0.96      0.98      0.97      3032\n",
      "\n",
      "    accuracy                           0.97      6064\n",
      "   macro avg       0.97      0.97      0.97      6064\n",
      "weighted avg       0.97      0.97      0.97      6064\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CELL 6 - Evaluate DistilBERT\n",
    "model.eval()\n",
    "preds = []\n",
    "true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        batch_preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "        preds.extend(batch_preds)\n",
    "        true.extend(batch['labels'].numpy())\n",
    "\n",
    "print(\"DistilBERT Accuracy:\", accuracy_score(true, preds))\n",
    "print(classification_report(true, preds, target_names=['Real', 'Fake']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a31c19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../backend/nlp/distilbert_fake_review\n"
     ]
    }
   ],
   "source": [
    "save_path = \"../backend/nlp/distilbert_fake_review\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb683676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe already exists!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.exists(\"../data/glove.6B.300d.txt\"):\n",
    "    print(\"Downloading GloVe 300d...\")\n",
    "    !wget -O ../data/glove.6B.zip https://nlp.stanford.edu/data/glove.6B.zip\n",
    "    !unzip ../data/glove.6B.zip -d ../data/\n",
    "    print(\"GloVe downloaded!\")\n",
    "else:\n",
    "    print(\"GloVe already exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93aaeefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Build vocab\n",
    "all_words = [word for text in df['cleaned'] for word in text.split()]\n",
    "vocab = Counter(all_words)\n",
    "vocab = ['<PAD>', '<UNK>'] + [word for word, count in vocab.most_common()]\n",
    "\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Load GloVe\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "glove_path = \"../data/glove.6B.300d.txt\"\n",
    "with open(glove_path, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if word in word_to_idx:\n",
    "            idx = word_to_idx[word]\n",
    "            embedding_matrix[idx] = np.array(values[1:], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dce44e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (48456, 300)\n"
     ]
    }
   ],
   "source": [
    "# For unknown words ‚Üí random\n",
    "for i in range(vocab_size):\n",
    "    if np.all(embedding_matrix[i] == 0):\n",
    "        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72f9653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMDataset(Dataset):\n",
    "    def __init__(self, texts, labels, word_to_idx, max_len=100):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        words = text.split()[:self.max_len]\n",
    "        indices = [self.word_to_idx.get(word, 1) for word in words]  # 1 = <UNK>\n",
    "        if len(indices) < self.max_len:\n",
    "            indices += [0] * (self.max_len - len(indices))  # 0 = <PAD>\n",
    "        \n",
    "        return {\n",
    "            'indices': torch.tensor(indices, dtype=torch.long),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21bd50ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim=128, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix), padding_idx=0\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=300,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=0.5\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        final_hidden = lstm_out[:, -1, :]  # Last timestep\n",
    "        out = self.dropout(final_hidden)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39b826dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "bilstm_train = BiLSTMDataset(X_train.tolist(), y_train.tolist(), word_to_idx)\n",
    "bilstm_val = BiLSTMDataset(X_val.tolist(), y_val.tolist(), word_to_idx)\n",
    "\n",
    "train_loader_bilstm = DataLoader(bilstm_train, batch_size=64, shuffle=True)\n",
    "val_loader_bilstm = DataLoader(bilstm_val, batch_size=64)\n",
    "\n",
    "# Model\n",
    "bilstm_model = BiLSTM(embedding_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(bilstm_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ce69ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BiLSTM + GloVe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 443/443 [02:42<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.6749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 443/443 [02:13<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 0.5811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 443/443 [01:43<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Loss: 0.3687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 443/443 [01:45<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Loss: 0.2524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 443/443 [01:45<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Loss: 0.2052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 443/443 [01:43<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Loss: 0.1655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 443/443 [01:41<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Loss: 0.1343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 443/443 [01:41<00:00,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Loss: 0.1175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training BiLSTM + GloVe...\")\n",
    "bilstm_model.train()\n",
    "for epoch in range(8):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader_bilstm):\n",
    "        optimizer.zero_grad()\n",
    "        indices = batch['indices'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = bilstm_model(indices)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss/len(train_loader_bilstm):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94bd1984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM model saved!\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "torch.save(bilstm_model.state_dict(), \"../backend/nlp/bilstm_fake_review.pth\")\n",
    "print(\"BiLSTM model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb52e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DistilBERT predictions (from earlier)\n",
    "distilbert_preds = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "621e00e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLSTM predictions\n",
    "bilstm_model.eval()\n",
    "bilstm_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader_bilstm:\n",
    "        indices = batch['indices'].to(device)\n",
    "        outputs = bilstm_model(indices)\n",
    "        pred = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        bilstm_preds.extend(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1788834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL COMPARISON ===\n",
      "DistilBERT Accuracy: 0.9736 | F1: 0.9739\n",
      "BiLSTM+GloVe Accuracy: 0.9309 | F1: 0.9303\n"
     ]
    }
   ],
   "source": [
    "print(\"=== MODEL COMPARISON ===\")\n",
    "print(f\"DistilBERT Accuracy: {accuracy_score(true, distilbert_preds):.4f} | F1: {f1_score(true, distilbert_preds):.4f}\")\n",
    "print(f\"BiLSTM+GloVe Accuracy: {accuracy_score(true, bilstm_preds):.4f} | F1: {f1_score(true, bilstm_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13e12558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENSEMBLE Accuracy: 0.9535 | F1: 0.9550\n"
     ]
    }
   ],
   "source": [
    "# Ensemble (simple voting)\n",
    "ensemble_preds = []\n",
    "for d, b in zip(distilbert_preds, bilstm_preds):\n",
    "    ensemble_preds.append(1 if (d + b) >= 1 else 0)  # majority vote\n",
    "\n",
    "print(f\"ENSEMBLE Accuracy: {accuracy_score(true, ensemble_preds):.4f} | F1: {f1_score(true, ensemble_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b0995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
