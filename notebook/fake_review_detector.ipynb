{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da2edc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8222a8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc2688bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             category  rating label  \\\n",
       "0  Home_and_Kitchen_5     5.0    CG   \n",
       "1  Home_and_Kitchen_5     5.0    CG   \n",
       "2  Home_and_Kitchen_5     5.0    CG   \n",
       "3  Home_and_Kitchen_5     1.0    CG   \n",
       "4  Home_and_Kitchen_5     5.0    CG   \n",
       "\n",
       "                                               text_  \n",
       "0  Love this!  Well made, sturdy, and very comfor...  \n",
       "1  love it, a great upgrade from the original.  I...  \n",
       "2  This pillow saved my back. I love the look and...  \n",
       "3  Missing information on how to use it, but it i...  \n",
       "4  Very nice set. Good quality. We have had the s...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/fake reviews dataset.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b0fe164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (40432, 4)\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "CG    20216\n",
      "OR    20216\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b995f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if col == \"text_\":\n",
    "        df = df.rename(columns={'text_':\"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34f48801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             category  rating label  \\\n",
       "0  Home_and_Kitchen_5     5.0    CG   \n",
       "1  Home_and_Kitchen_5     5.0    CG   \n",
       "2  Home_and_Kitchen_5     5.0    CG   \n",
       "3  Home_and_Kitchen_5     1.0    CG   \n",
       "4  Home_and_Kitchen_5     5.0    CG   \n",
       "\n",
       "                                                text  \n",
       "0  Love this!  Well made, sturdy, and very comfor...  \n",
       "1  love it, a great upgrade from the original.  I...  \n",
       "2  This pillow saved my back. I love the look and...  \n",
       "3  Missing information on how to use it, but it i...  \n",
       "4  Very nice set. Good quality. We have had the s...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7c7d75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text', 'label']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57a8d539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower()) #- any character that is NOT a letter (a‚Äìz, A‚ÄìZ) or whitespace. - Those characters are replaced with '' \n",
    "    text = re.sub(r'\\s+', ' ', text) #- one or more whitespace characters.- Replaces them with a single space\n",
    "    return text.strip()\n",
    "\n",
    "df['cleaned'] = df['text'].apply(clean_text) #- Applies your clean_text function to each row.\n",
    "df = df[df['cleaned'].str.len() > 10] #removing trivial, noisy, or uninformative samples that could hurt model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ff8e4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "      <td>CG</td>\n",
       "      <td>love this well made sturdy and very comfortabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "      <td>CG</td>\n",
       "      <td>love it a great upgrade from the original ive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "      <td>CG</td>\n",
       "      <td>this pillow saved my back i love the look and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "      <td>CG</td>\n",
       "      <td>missing information on how to use it but it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "      <td>CG</td>\n",
       "      <td>very nice set good quality we have had the set...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label  \\\n",
       "0  Love this!  Well made, sturdy, and very comfor...    CG   \n",
       "1  love it, a great upgrade from the original.  I...    CG   \n",
       "2  This pillow saved my back. I love the look and...    CG   \n",
       "3  Missing information on how to use it, but it i...    CG   \n",
       "4  Very nice set. Good quality. We have had the s...    CG   \n",
       "\n",
       "                                             cleaned  \n",
       "0  love this well made sturdy and very comfortabl...  \n",
       "1  love it a great upgrade from the original ive ...  \n",
       "2  this pillow saved my back i love the look and ...  \n",
       "3  missing information on how to use it but it is...  \n",
       "4  very nice set good quality we have had the set...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b783f98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40430, 3)\n"
     ]
    }
   ],
   "source": [
    "df['label'] = df['label'].map({'CG': 1, 'OR':0})\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0287e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40430,)\n",
      "(40430,)\n"
     ]
    }
   ],
   "source": [
    "X = df['cleaned']\n",
    "y = df['label']\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc05a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 28301, Val: 6064, Test: 6065\n"
     ]
    }
   ],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X,y,test_size=0.3,random_state=42, stratify=y)\n",
    "\n",
    "X_val,X_test,y_val,y_test = train_test_split(X_temp,y_temp,test_size=0.5,random_state=42,stratify=y_temp)\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8899ac9d",
   "metadata": {},
   "source": [
    "DistilBERT is a distilled version of BERT: it keeps the same Transformer encoder ideas but with fewer layers so it runs faster and uses less memory. It learns language patterns from large text corpora and can be fine‚Äëtuned for tasks like classification. The Hugging Face DistilBertForSequenceClassification wraps the encoder and adds a small classification head on top to output logits for each class.\n",
    "\n",
    "Architecture in simple terms\n",
    "- Transformer encoder: reads the whole sentence and builds contextual word vectors (each token‚Äôs meaning depends on the whole sentence).\n",
    "- Distillation: a teacher model (BERT) teaches a smaller student (DistilBERT) to mimic its behavior so the student is compact but still strong.\n",
    "- Classification head: a linear layer on top of the pooled output that maps to num_labels (here 2).\n",
    "\n",
    "\n",
    "üîπ What is PyTorch actually doing?\n",
    "- PyTorch is a deep learning framework. Think of it as the ‚Äúengine‚Äù that lets us build, train, and run neural networks.\n",
    "- It provides:\n",
    "- Tensors (its basic data structure, like NumPy arrays but with GPU support).\n",
    "- Automatic differentiation (so gradients for backpropagation are calculated automatically).\n",
    "- Modules (ready‚Äëmade building blocks like layers, optimizers, losses).\n",
    "- In your code, PyTorch is handling:\n",
    "- The dataset (Dataset and DataLoader classes).\n",
    "- The training loop (forward pass, loss calculation, backward pass, optimizer step).\n",
    "- Moving data and models to GPU/CPU (.to(device)).\n",
    "\n",
    "üîπ What are Tensors?\n",
    "- A tensor is just a multi‚Äëdimensional array (like a matrix, but more general).\n",
    "- Example:\n",
    "- Scalar ‚Üí 5 (0D tensor).\n",
    "- Vector ‚Üí [1,2,3] (1D tensor).\n",
    "- Matrix ‚Üí [[1,2],[3,4]] (2D tensor).\n",
    "- Higher dimensions ‚Üí images, batches of text, etc.\n",
    "- PyTorch tensors can live on GPU, which makes training super fast compared to normal Python arrays.\n",
    "\n",
    "üîπ What is the Attention Mask?\n",
    "- When we tokenize text, we pad shorter sentences to a fixed length (say 256 tokens).\n",
    "- Example: \"hello world\" ‚Üí [101, 7592, 2088, 102, 0, 0, 0...] (zeros are padding).\n",
    "- The attention mask tells the model which tokens are real and which are just padding.\n",
    "- 1 ‚Üí real token.\n",
    "- 0 ‚Üí padding.\n",
    "- Without this, the model would waste attention on meaningless padding tokens.\n",
    "\n",
    " Why do we need tensors?\n",
    "- Deep learning models (like DistilBERT) are basically giant math machines.\n",
    "- They don‚Äôt understand text directly ‚Äî they understand numbers.\n",
    "- A tensor is the data structure PyTorch uses to hold these numbers.\n",
    "- Think of a tensor as a container for numbers that can be:\n",
    "- 1D (like a list of token IDs for one sentence).\n",
    "- 2D (like a batch of sentences, each with token IDs).\n",
    "- 3D+ (images, video frames, etc.).\n",
    "The key advantage: tensors can live on the GPU, so millions of operations can be done in parallel very fast. Without tensors, training would be painfully slow.\n",
    "\n",
    "üîπ What help are tensors doing?\n",
    "- They allow PyTorch to:\n",
    "- Store the tokenized text (numbers instead of words).\n",
    "- Store the attention mask (1s and 0s for real vs. padded tokens).\n",
    "- Store the labels (like 0 for \"Fake\", 1 for \"Real\").\n",
    "- During training:\n",
    "- These tensors are fed into the model.\n",
    "- The model does matrix multiplications and attention calculations on them.\n",
    "- Gradients (also tensors) are computed and used to update weights.\n",
    "So tensors are the bridge between your text data and the math inside the neural network.\n",
    "\n",
    "üîπ Why is the dataset being sent as a list?\n",
    "When you create ReviewDataset(X_train.tolist(), y_train.tolist(), tokenizer):\n",
    "- X_train and y_train are often NumPy arrays or Pandas series.\n",
    "- .tolist() converts them into Python lists because the custom Dataset class expects normal lists it can index with __getitem__.\n",
    "- Lists are easier for PyTorch‚Äôs Dataset to handle when fetching samples one by one.\n",
    "So: lists = simple containers of text and labels ‚Üí dataset wraps them ‚Üí DataLoader batches them.\n",
    "\n",
    "üîπ What is the DataLoader and what is it doing?\n",
    "Think of DataLoader as a waiter in a restaurant:\n",
    "- The Dataset is the kitchen (it has all the food = samples).\n",
    "- The DataLoader is the waiter who brings food to the table in small batches.\n",
    "- The Model is the customer who eats the food (learns from the data).\n",
    "What DataLoader does:\n",
    "- Batching: Instead of giving the model one sentence at a time, it gives (say) 16 sentences together. This speeds up training and stabilizes learning.\n",
    "- Shuffling: Randomizes the order of samples each epoch so the model doesn‚Äôt memorize sequence patterns.\n",
    "- Parallel loading: Can fetch data using multiple workers (threads) to keep GPU busy.\n",
    "So DataLoader = efficient delivery system for data ‚Üí model.\n",
    "\n",
    "üîπ What is AdamW and how does it optimize the model?\n",
    "Optimization = how the model learns by adjusting its weights.\n",
    "AdamW in simple terms:\n",
    "- Adam = Adaptive Moment Estimation. It‚Äôs like a smart gradient descent:\n",
    "- It looks at the gradient (direction of error).\n",
    "- It keeps track of past gradients (momentum) so it doesn‚Äôt zig‚Äëzag too much.\n",
    "- It adapts the learning rate for each parameter individually.\n",
    "- W = Weight Decay. This prevents weights from growing too large (regularization).\n",
    "What happens during optimization:\n",
    "- Model makes a prediction ‚Üí compares with true label ‚Üí computes loss.\n",
    "- PyTorch calculates gradients (how much each weight contributed to the error).\n",
    "- AdamW uses these gradients to adjust weights slightly in the right direction.\n",
    "- Example: if a weight made the model predict too high, AdamW nudges it lower.\n",
    "- Repeat for many batches ‚Üí model gradually learns patterns in the data.\n",
    "\n",
    "üîπ Putting it all together (flow)\n",
    "- Dataset ‚Üí holds text + labels.\n",
    "- DataLoader ‚Üí batches them into tensors.\n",
    "- Model ‚Üí takes tensors, runs forward pass, outputs predictions.\n",
    "- Loss ‚Üí measures how wrong predictions are.\n",
    "- Backpropagation ‚Üí computes gradients.\n",
    "- AdamW optimizer ‚Üí updates weights using gradients.\n",
    "- Repeat for many epochs ‚Üí model improves accuracy.\n",
    "\n",
    "‚ö° Simple analogy:\n",
    "- Dataset = library of books.\n",
    "- DataLoader = librarian who brings 16 books at a time.\n",
    "- Model = student reading books.\n",
    "- Loss = exam score showing mistakes.\n",
    "- AdamW = teacher correcting the student‚Äôs notes so they improve next time.\n",
    "\n",
    "\n",
    "## üîπ Why do we need the loop for each epoch?\n",
    "Let‚Äôs break it down:\n",
    "\n",
    "### 1. `model.train()` alone is not enough\n",
    "- `model.train()` just **switches the model into training mode** (turns on dropout, etc.).  \n",
    "- It does **not** actually train the model.  \n",
    "- Training requires:  \n",
    "  - Feeding data (forward pass).  \n",
    "  - Calculating loss.  \n",
    "  - Backpropagation (gradients).  \n",
    "  - Optimizer step (update weights).  \n",
    "- That‚Äôs why we need the loop ‚Äî to actually perform these steps repeatedly.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. The **epoch loop** ensures repeated exposure\n",
    "- One epoch = model sees the entire dataset once.  \n",
    "- But one pass is not enough ‚Äî the model won‚Äôt learn well from just one look.  \n",
    "- Multiple epochs = repeated practice.  \n",
    "- Each epoch refines the weights further, reducing loss and improving accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. The **batch loop** inside each epoch\n",
    "- Datasets are too big to feed all at once (memory issue).  \n",
    "- So we split into **mini‚Äëbatches** (e.g., 16 samples).  \n",
    "- For each batch:\n",
    "  1. Forward pass ‚Üí model predicts.  \n",
    "  2. Loss ‚Üí compare prediction vs. true labels.  \n",
    "  3. Backward pass ‚Üí compute gradients.  \n",
    "  4. Optimizer step ‚Üí update weights.  \n",
    "- Repeat until all batches are done ‚Üí that completes one epoch.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Why can‚Äôt we just do one epoch?\n",
    "- Imagine learning multiplication tables:\n",
    "  - If you study them once, you‚Äôll forget quickly.  \n",
    "  - If you repeat them multiple times, you get better and faster.  \n",
    "- Same with the model:  \n",
    "  - One epoch = rough first attempt.  \n",
    "  - More epochs = gradual refinement.  \n",
    "  - Too many epochs = risk of memorizing (overfitting).  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Full training loop explained in simple flow\n",
    "1. **Set training mode** ‚Üí `model.train()`.  \n",
    "2. **Epoch loop** ‚Üí repeat training passes (e.g., 3 times).  \n",
    "3. **Batch loop** ‚Üí for each mini‚Äëbatch:  \n",
    "   - Get tensors (`input_ids`, `attention_mask`, `labels`).  \n",
    "   - Forward pass ‚Üí predictions.  \n",
    "   - Loss ‚Üí how wrong predictions are.  \n",
    "   - Backward pass ‚Üí compute gradients.  \n",
    "   - Optimizer step ‚Üí update weights.  \n",
    "4. **Print loss** ‚Üí see how much error remains after each epoch.  \n",
    "\n",
    "---\n",
    "\n",
    "‚ö° **Analogy:**  \n",
    "Think of training like practicing basketball free throws:  \n",
    "- **Batch loop** = each shot you take.  \n",
    "- **Epoch loop** = one full practice session (all shots).  \n",
    "- **Multiple epochs** = multiple practice sessions over days.  \n",
    "- Just saying ‚ÄúI‚Äôm practicing‚Äù (`model.train()`) doesn‚Äôt make you better ‚Äî you actually need to take the shots (the loops).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d81915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset): #defines a PyTorch dataset wrapperdefines a PyTorch dataset wrapper\n",
    "    def __init__(self,texts,labels,tokenizer,max_len=256): #- stores texts, labels, the tokenizer, and a max length for padding/truncation.\n",
    "        self.texts = texts \n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer \n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self): # tells PyTorch how many samples exist.\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self ,idx):\n",
    "        text = str(self.texts[idx]) #fetch raw text and label. #- idx is the index of the sample to retrieve.\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer( #- tokenizer converts text to input_ids and attention_mask tensors; truncation and padding='max_length' ensure fixed length\n",
    "            text,\n",
    "            truncation = True,\n",
    "            padding='max_length',# Cut off if longer than max_len\n",
    "            max_length=self.max_len,\n",
    "            return_tensors ='pt'# Return PyTorch tensors\n",
    "        )\n",
    "        return{\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9cf1eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\Coding Workspaces\\RepuTrack\\.venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased') #downloads tokenizer vocab and rules\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2) #downloads pre-trained DistilBERT model with a classification head for 2 classes\n",
    "model.to(device) # moves model to GPU/CPU\n",
    "\n",
    "#Dataset \n",
    "train_dataset = ReviewDataset(X_train.tolist(), y_train.tolist(), tokenizer)\n",
    "val_dataset = ReviewDataset(X_val.tolist(), y_val.tolist(), tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset , batch_size = 16, shuffle= True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b57cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DistilBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1769/1769 [3:32:42<00:00,  7.21s/it]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.1495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1769/1769 [3:04:04<00:00,  6.24s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 0.0510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1769/1769 [2:27:07<00:00,  4.99s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Loss: 0.0251\n",
      "DistilBERT Training Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer model    \n",
    "\n",
    "model.train()\n",
    "\n",
    "print(\"Training DistilBERT...\")\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader): # - tqdm is just a progress bar library in Python.\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"DistilBERT Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c0e4a",
   "metadata": {},
   "source": [
    "\n",
    "## üîπ First, the big picture\n",
    "In **machine learning (ML)** you usually do:\n",
    "- Train on **X_train, y_train**  \n",
    "- Test on **X_test, y_test**  \n",
    "- Compare predictions (`y_pred`) with true labels (`y_test`)  \n",
    "\n",
    "In **PyTorch NLP**, the idea is the same ‚Äî but instead of plain arrays, we use **tensors + DataLoader** to feed the model.  \n",
    "\n",
    "So evaluation = **feed unseen data into the trained model ‚Üí collect predictions ‚Üí compare with true labels ‚Üí compute accuracy/metrics.**\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Your evaluation code\n",
    "```python\n",
    "model.eval()\n",
    "preds = []\n",
    "true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        batch_preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "        preds.extend(batch_preds)\n",
    "        true.extend(batch['labels'].numpy())\n",
    "\n",
    "print(\"DistilBERT Accuracy:\", accuracy_score(true, preds))\n",
    "print(classification_report(true, preds, target_names=['Real', 'Fake']))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step by step explanation\n",
    "\n",
    "### 1. `model.eval()`\n",
    "- Switches the model into **evaluation mode**.  \n",
    "- This turns off things like **dropout** (randomly dropping neurons during training).  \n",
    "- Ensures predictions are stable and consistent.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `with torch.no_grad():`\n",
    "- Tells PyTorch: ‚ÄúDon‚Äôt calculate gradients now.‚Äù  \n",
    "- Why? Because during evaluation we don‚Äôt need backpropagation ‚Äî we‚Äôre just checking performance.  \n",
    "- Saves memory and speeds things up.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Looping through `val_loader`\n",
    "- `val_loader` is the DataLoader for your **validation set** (X_val, y_val).  \n",
    "- It gives batches of tokenized text + labels.  \n",
    "- For each batch:\n",
    "  - `input_ids` ‚Üí tokenized text.  \n",
    "  - `attention_mask` ‚Üí tells model which tokens are padding.  \n",
    "  - `labels` ‚Üí true class (0 or 1).  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Forward pass (getting predictions)\n",
    "```python\n",
    "outputs = model(input_ids, attention_mask=attention_mask)\n",
    "```\n",
    "- The model processes the batch and outputs **logits** (raw scores for each class).  \n",
    "- Example: for binary classification, logits might look like `[2.3, -1.1]` ‚Üí meaning class 0 is more likely.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Converting logits to predictions\n",
    "```python\n",
    "batch_preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "```\n",
    "- `torch.argmax(..., dim=1)` ‚Üí picks the class with the highest score.  \n",
    "- Converts tensor ‚Üí NumPy array ‚Üí easy to store.  \n",
    "- Example: `[2.3, -1.1]` ‚Üí prediction = `0`.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Collecting predictions and true labels\n",
    "```python\n",
    "preds.extend(batch_preds)\n",
    "true.extend(batch['labels'].numpy())\n",
    "```\n",
    "- `preds` = all predicted labels across batches.  \n",
    "- `true` = all actual labels across batches.  \n",
    "- At the end, you have two lists: just like `y_pred` and `y_test` in ML.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Computing metrics\n",
    "```python\n",
    "print(\"DistilBERT Accuracy:\", accuracy_score(true, preds))\n",
    "print(classification_report(true, preds, target_names=['Real', 'Fake']))\n",
    "```\n",
    "- **Accuracy** = fraction of correct predictions.  \n",
    "- **Classification report** = precision, recall, F1‚Äëscore for each class (`Real`, `Fake`).  \n",
    "- This is exactly like scikit‚Äëlearn evaluation, just with tensors ‚Üí arrays conversion.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Why 3 splits (train, val, test)?\n",
    "- **Train set (X_train, y_train)** ‚Üí used to teach the model (update weights).  \n",
    "- **Validation set (X_val, y_val)** ‚Üí used during training to check performance and tune hyperparameters (like learning rate, batch size).  \n",
    "- **Test set (X_test, y_test)** ‚Üí final unseen data to measure how well the model generalizes.  \n",
    "\n",
    "üëâ In your code, they used **train + val**. Often after training, you‚Äôd also evaluate on **test** to report final accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "‚ö° **Analogy:**  \n",
    "- **Train set** = practice questions.  \n",
    "- **Validation set** = mock exam to check progress.  \n",
    "- **Test set** = real exam to measure final ability.  \n",
    "- Evaluation loop = grading the exam: collect answers (preds), compare with correct answers (true), compute score (accuracy/F1).\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ So evaluation in PyTorch NLP is the same idea as ML:  \n",
    "- Get predictions ‚Üí compare with true labels ‚Üí compute metrics.  \n",
    "- The only difference is the **DataLoader + tensors** machinery feeding the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b63e7914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT Accuracy: 0.9736147757255936\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.98      0.96      0.97      3032\n",
      "        Fake       0.96      0.98      0.97      3032\n",
      "\n",
      "    accuracy                           0.97      6064\n",
      "   macro avg       0.97      0.97      0.97      6064\n",
      "weighted avg       0.97      0.97      0.97      6064\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CELL 6 - Evaluate DistilBERT\n",
    "model.eval()\n",
    "preds = []\n",
    "true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        batch_preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "        preds.extend(batch_preds)\n",
    "        true.extend(batch['labels'].numpy())\n",
    "\n",
    "print(\"DistilBERT Accuracy:\", accuracy_score(true, preds))\n",
    "print(classification_report(true, preds, target_names=['Real', 'Fake']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a31c19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../backend/nlp/distilbert_fake_review\n"
     ]
    }
   ],
   "source": [
    "save_path = \"../backend/nlp/distilbert_fake_review\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb683676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe already exists!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.exists(\"../data/glove.6B.300d.txt\"):\n",
    "    print(\"Downloading GloVe 300d...\")\n",
    "    !wget -O ../data/glove.6B.zip https://nlp.stanford.edu/data/glove.6B.zip\n",
    "    !unzip ../data/glove.6B.zip -d ../data/\n",
    "    print(\"GloVe downloaded!\")\n",
    "else:\n",
    "    print(\"GloVe already exists!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de905a5",
   "metadata": {},
   "source": [
    "When we train a BiLSTM (Bidirectional LSTM) for text tasks (like sentiment analysis, classification, etc.), the model needs numerical input. Words are text, but neural networks only understand numbers.\n",
    "So we:\n",
    "- Build a vocabulary ‚Üí assign each word an index.\n",
    "- Load GloVe embeddings ‚Üí instead of random numbers, we give each word a pretrained vector that already encodes meaning.\n",
    "This way, the BiLSTM doesn‚Äôt start from scratch ‚Äî it already knows that ‚Äúking‚Äù is close to ‚Äúqueen‚Äù, or ‚Äúhappy‚Äù is opposite of ‚Äúsad‚Äù in vector space.\n",
    "\n",
    "- GloVe (Global Vectors for Word Representation):\n",
    "Pretrained embeddings learned from billions of words. They capture semantic relationships.\n",
    "Example: vector(‚ÄúParis‚Äù) ‚Äì vector(‚ÄúFrance‚Äù) + vector(‚ÄúItaly‚Äù) ‚âà vector(‚ÄúRome‚Äù).\n",
    "- BiLSTM (Bidirectional Long Short-Term Memory):\n",
    "A sequence model that reads text both forward and backward.\n",
    "- Forward LSTM: captures context from left to right.\n",
    "- Backward LSTM: captures context from right to left.\n",
    "Together, they understand meaning in context better than a one-directional model.\n",
    "üëâ Combining them:\n",
    "- GloVe gives word-level meaning.\n",
    "- BiLSTM adds sentence-level context.\n",
    "This is powerful for tasks like sentiment analysis, intent detection, or text classification.\n",
    "\n",
    "üìñ Small Example (no code, just concept)\n",
    "Imagine the sentence:\n",
    "‚ÄúThe movie was not good.‚Äù\n",
    "- Step 1: Vocabulary + GloVe\n",
    "Each word gets a vector:\n",
    "- ‚Äúmovie‚Äù ‚Üí [0.12, -0.45, ‚Ä¶]\n",
    "- ‚Äúgood‚Äù ‚Üí [0.88, 0.34, ‚Ä¶]\n",
    "- ‚Äúnot‚Äù ‚Üí [0.55, -0.22, ‚Ä¶]\n",
    "- Step 2: BiLSTM reads the sequence\n",
    "- Forward LSTM sees: ‚ÄúThe ‚Üí movie ‚Üí was ‚Üí not ‚Üí good‚Äù\n",
    "- Backward LSTM sees: ‚Äúgood ‚Üí not ‚Üí was ‚Üí movie ‚Üí The‚Äù\n",
    "- Step 3: Context understanding\n",
    "- Forward path might think ‚Äúgood‚Äù is positive.\n",
    "- Backward path sees ‚Äúnot‚Äù before ‚Äúgood‚Äù and adjusts meaning.\n",
    "- Together, BiLSTM understands the sentence is negative sentiment.\n",
    "So the GloVe embeddings give the model a head start on what ‚Äúgood‚Äù and ‚Äúnot‚Äù mean, and the BiLSTM ensures the order and context are correctly interpreted.\n",
    "\n",
    "‚ú® In short:\n",
    "We use GloVe so words start with rich semantic meaning, and BiLSTM so the model understands how those words interact in context.\n",
    "Would you like me to also show you a visual analogy (like a diagram-style explanation of how embeddings flow into BiLSTM)? That often makes the ‚Äúwhy‚Äù click even faster.\n",
    "\n",
    "\n",
    "## üß© Step 1: Making a Vocabulary\n",
    "Imagine you have a dataset of cleaned sentences, like:\n",
    "\n",
    "```\n",
    "df['cleaned'] =\n",
    "[\"the movie was good\",\n",
    " \"the movie was not good\",\n",
    " \"the actor was amazing\"]\n",
    "```\n",
    "\n",
    "- **`all_words`** ‚Üí collects every word:  \n",
    "  `[\"the\",\"movie\",\"was\",\"good\",\"the\",\"movie\",\"was\",\"not\",\"good\",\"the\",\"actor\",\"was\",\"amazing\"]`\n",
    "\n",
    "- **`Counter(all_words)`** ‚Üí counts frequency:  \n",
    "  ```\n",
    "  the: 3\n",
    "  movie: 2\n",
    "  was: 3\n",
    "  good: 2\n",
    "  not: 1\n",
    "  actor: 1\n",
    "  amazing: 1\n",
    "  ```\n",
    "\n",
    "- **`vocab = ['<PAD>', '<UNK>'] + [word for word,count in vocab.most_common()]`**  \n",
    "  Adds special tokens first, then words sorted by frequency:  \n",
    "  ```\n",
    "  ['<PAD>', '<UNK>', 'the', 'was', 'movie', 'good', 'not', 'actor', 'amazing']\n",
    "  ```\n",
    "\n",
    "üëâ This is your **vocabulary list**.\n",
    "\n",
    "---\n",
    "What are <PAD> and <UNK> used for?\n",
    "- <PAD> (Padding token):\n",
    "- Neural networks usually expect all input sequences to be the same length.\n",
    "- But sentences vary: \"the movie was good\" has 4 words, \"the actor was amazing\" has 4, \"the movie was not good\" has 5.\n",
    "- To make them equal length, we add padding tokens at the end (or beginning).\n",
    "- Example: if we fix length = 6, then:\n",
    "- \"the movie was good\" ‚Üí [2,4,3,5,0,0] (where 0 = <PAD>).\n",
    "- Padding ensures the model can batch-process sentences together.\n",
    "- <UNK> (Unknown token):\n",
    "- Sometimes a word appears in your dataset that wasn‚Äôt in your vocabulary or GloVe file.\n",
    "- Example: \"blockbuster\" might not be in your vocab.\n",
    "- Instead of crashing, the model replaces it with <UNK> (index 1).\n",
    "- This way, the model can still handle unseen words gracefully.\n",
    "üëâ Together, <PAD> and <UNK> make the system robust:\n",
    "- <PAD> = ‚Äúempty filler‚Äù to align sequence lengths.\n",
    "- <UNK> = ‚Äúcatch-all‚Äù for unknown words.\n",
    "\n",
    "## üî¢ Step 2: Turning Words into Numbers\n",
    "- **`word_to_idx`** ‚Üí assigns each word an index:  \n",
    "  ```\n",
    "  {\n",
    "   '<PAD>':0, '<UNK>':1,\n",
    "   'the':2, 'was':3, 'movie':4,\n",
    "   'good':5, 'not':6, 'actor':7, 'amazing':8\n",
    "  }\n",
    "  ```\n",
    "\n",
    "So now:\n",
    "- Sentence `\"the movie was good\"` becomes `[2,4,3,5]`.  \n",
    "- Sentence `\"the movie was not good\"` becomes `[2,4,3,6,5]`.\n",
    "\n",
    "üëâ That‚Äôs how text becomes **numerical values** (indices).\n",
    "\n",
    "---\n",
    "\n",
    "## üìê Step 3: What is a Vector?\n",
    "A **vector** here means a list of numbers (not just one number).  \n",
    "- Each word is represented by a **300-dimensional vector** (300 numbers).  \n",
    "- Example (simplified):  \n",
    "  `\"good\"` ‚Üí `[0.88, 0.34, -0.12, ‚Ä¶ up to 300 values]`  \n",
    "  `\"bad\"` ‚Üí `[0.12, -0.56, 0.77, ‚Ä¶]`\n",
    "\n",
    "üëâ So it‚Äôs not a single number, it‚Äôs a **long list of numbers** that encodes meaning.\n",
    "\n",
    "---\n",
    "\n",
    "## üìÇ Step 4: Reading the GloVe File\n",
    "The GloVe file (`glove.6B.300d.txt`) is a huge text file where each line looks like:\n",
    "\n",
    "```\n",
    "good 0.88 0.34 -0.12 ... (300 numbers)\n",
    "bad 0.12 -0.56 0.77 ... (300 numbers)\n",
    "movie 0.45 -0.22 0.91 ... (300 numbers)\n",
    "```\n",
    "\n",
    "- The **first word** is the token (`good`, `bad`, etc.).  \n",
    "- The **next 300 numbers** are its vector.  \n",
    "\n",
    "The code reads each line, checks if the word is in your vocabulary, and if yes ‚Üí puts its vector into the `embedding_matrix` at the right index.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Step 5: Why 300?\n",
    "- **300** is the embedding dimension chosen when GloVe was trained.  \n",
    "- It means each word is represented by a vector of length 300.  \n",
    "- More dimensions = more expressive meaning, but also heavier computation.  \n",
    "- 300 is a sweet spot: captures rich semantic info without being too large.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Step 6: How it Helps\n",
    "- Your vocabulary gives each word an **index**.  \n",
    "- GloVe gives each word a **vector of meaning**.  \n",
    "- Together, you get an **embedding matrix**:  \n",
    "  - Row 2 (for ‚Äúthe‚Äù) ‚Üí vector of 300 numbers.  \n",
    "  - Row 5 (for ‚Äúgood‚Äù) ‚Üí vector of 300 numbers.  \n",
    "\n",
    "When the BiLSTM runs, instead of just seeing `[2,4,3,5]` (plain numbers), it sees the **embedding vectors** that carry semantic meaning. That‚Äôs why the model can understand ‚Äúgood‚Äù vs ‚Äúbad‚Äù beyond just their IDs.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **In short:**  \n",
    "- Vocabulary = word ‚Üí index.  \n",
    "- GloVe file = word ‚Üí 300-number vector.  \n",
    "- Code combines them into `embedding_matrix`.  \n",
    "- BiLSTM uses this matrix so words start with rich semantic meaning, not random numbers.  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aaeefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Build vocab\n",
    "all_words = [word for text in df['cleaned'] for word in text.split()] #- Flattens every token from every cleaned text into a single list. Assumes df['cleaned'] contains whitespace-tokenized strings (already preprocessed).\n",
    "vocab = Counter(all_words) #\n",
    "vocab = ['<PAD>', '<UNK>'] + [word for word, count in vocab.most_common()] #- Creates the final vocabulary list with two special tokens: <PAD> (for sequence padding) and <UNK> (unknown/out-of-vocab words). Then appends words sorted by descending frequency using most_common().\n",
    "\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)} #- Builds a dictionary mapping each token to a unique integer index. Index 0 will be <PAD>, index 1 <UNK> because of the list order.\n",
    "vocab_size = len(vocab) #Stores vocabulary size for later use (embedding matrix dimensions, model input layers).\n",
    "\n",
    "# Load GloVe\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim)) #- Initializes a matrix of zeros. Row i will hold the vector for the word whose index is i. Zero vectors remain for words not found in GloVe \n",
    "\n",
    "glove_path = \"../data/glove.6B.300d.txt\"\n",
    "with open(glove_path, encoding='utf-8') as f:\n",
    "    for line in f: #- Reads GloVe file line by line.\n",
    "         #- For each line it: - Splits into tokens, takes the first token as word and the rest as the vector values.\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if word in word_to_idx: #- If word exists in word_to_idx, it converts the numeric strings to a float32 numpy array and assigns it to the corresponding row in embedding_matrix.\n",
    "            idx = word_to_idx[word]\n",
    "            embedding_matrix[idx] = np.array(values[1:], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56aabfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_matrix.npy saved!\n"
     ]
    }
   ],
   "source": [
    "np.save(\"../backend/nlp/embedding_matrix.npy\", embedding_matrix)\n",
    "print(\"embedding_matrix.npy saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce44e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (48456, 300)\n"
     ]
    }
   ],
   "source": [
    "# For unknown words ‚Üí random\n",
    "for i in range(vocab_size): #- each row i in embedding_matrix corresponds to one word.\n",
    "    if np.all(embedding_matrix[i] == 0): #- This checks whether the row for word i is all zeros.\n",
    "        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "        \n",
    "# - If the word wasn‚Äôt found in GloVe, we don‚Äôt want it to stay as a boring zero vector (all words would look identical).\n",
    "# - So we give it a random vector drawn from a normal distribution (bell curve).\n",
    "# - scale=0.6 controls the spread of values (how ‚Äúwide‚Äù the bell curve is).\n",
    "# - size=(embedding_dim,) means we generate 300 numbers (since embedding_dim=300).\n",
    "# üëâ This way, every word has a unique vector, even if GloVe didn‚Äôt know it.\n",
    "\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6af809",
   "metadata": {},
   "source": [
    "How the Model Understands\n",
    "Here‚Äôs the magic:\n",
    "- Each vector places the word in a semantic space.\n",
    "- Words with similar meanings have vectors that are close together.\n",
    "- ‚Äúgood‚Äù and ‚Äúgreat‚Äù ‚Üí vectors are near each other.\n",
    "- ‚Äúgood‚Äù and ‚Äúbad‚Äù ‚Üí vectors are far apart.\n",
    "The BiLSTM doesn‚Äôt ‚Äúread‚Äù the word itself ‚Äî it processes the pattern of numbers in the vector.\n",
    "- If ‚Äúgood‚Äù has [0.8, 0.3, -0.1] and ‚Äúgreat‚Äù has [0.79, 0.31, -0.09], the model learns they mean something similar.\n",
    "- If ‚Äúnot‚Äù comes before ‚Äúgood,‚Äù the BiLSTM sees the sequence and adjusts the meaning (negative sentiment).\n",
    "\n",
    "üß© Example in Action\n",
    "Sentence: ‚ÄúThe movie was not good.‚Äù\n",
    "- Embedding lookup gives vectors for each word.\n",
    "- BiLSTM reads forward: ‚ÄúThe ‚Üí movie ‚Üí was ‚Üí not ‚Üí good‚Äù\n",
    "- BiLSTM reads backward: ‚Äúgood ‚Üí not ‚Üí was ‚Üí movie ‚Üí The‚Äù\n",
    "- Because ‚Äúnot‚Äù and ‚Äúgood‚Äù vectors are close together in the sequence, the BiLSTM learns that the overall meaning is negative, even though ‚Äúgood‚Äù alone is positive.\n",
    "\n",
    "‚úÖ In short\n",
    "- Row = vector of numbers for a word.\n",
    "- Vector = captures meaning and relationships.\n",
    "- Model understands words by processing these vectors in context.\n",
    "- GloVe gives the model a head start: instead of random numbers, words already have meaningful positions in vector space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f9653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMDataset(Dataset):#PyTorch models expect data in a certain format (tensors). This class converts raw text + labels into that format.\n",
    "    def __init__(self, texts, labels, word_to_idx, max_len=100):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts) #tells PyTorch how many samples are in the dataset.\n",
    "\n",
    "    def __getitem__(self, idx):#- given an index, returns:- indices: the sentence converted into word indices (with <UNK> for unknown words and <PAD> for padding).- label: the target class (e.g., 0 = negative, 1 = positive).\n",
    "\n",
    "        text = self.texts[idx]\n",
    "        words = text.split()[:self.max_len]\n",
    "        indices = [self.word_to_idx.get(word, 1) for word in words]  # 1 = <UNK>\n",
    "        if len(indices) < self.max_len:\n",
    "            indices += [0] * (self.max_len - len(indices))  # 0 = <PAD>\n",
    "        \n",
    "        return {\n",
    "            'indices': torch.tensor(indices, dtype=torch.long),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0e2b9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_to_idx.pkl saved!\n"
     ]
    }
   ],
   "source": [
    "with open(\"../backend/nlp/word_to_idx.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word_to_idx, f)\n",
    "print(\"word_to_idx.pkl saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629a3675",
   "metadata": {},
   "source": [
    "\n",
    "This class defines the **architecture** of the model ‚Äî the layers and how data flows through them.\n",
    "\n",
    "### 1. **Embedding Layer**\n",
    "```python\n",
    "self.embedding = nn.Embedding.from_pretrained(\n",
    "    torch.FloatTensor(embedding_matrix), padding_idx=0\n",
    ")\n",
    "```\n",
    "- Converts word indices into GloVe vectors.  \n",
    "- Example: index `5` ‚Üí vector `[0.77, 0.11, -0.55, ‚Ä¶]`.  \n",
    "- `padding_idx=0` ensures `<PAD>` always maps to a zero vector.\n",
    "\n",
    "### 2. **LSTM Layer**\n",
    "```python\n",
    "self.lstm = nn.LSTM(\n",
    "    input_size=300,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    bidirectional=True,\n",
    "    batch_first=True,\n",
    "    dropout=0.5\n",
    ")\n",
    "```\n",
    "- Input size = 300 (because GloVe vectors are 300-dimensional).  \n",
    "- Hidden size = 128 (the size of the LSTM‚Äôs memory).  \n",
    "- Num layers = 2 (stacked LSTMs for deeper learning).  \n",
    "- Bidirectional = True ‚Üí reads text forward and backward.  \n",
    "- Dropout = 0.5 ‚Üí prevents overfitting.\n",
    "\n",
    "### 3. **Fully Connected Layer**\n",
    "```python\n",
    "self.fc = nn.Linear(hidden_dim * 2, 2)\n",
    "```\n",
    "- Takes the final hidden state and maps it to 2 outputs (binary classification: e.g., positive vs negative).  \n",
    "- `hidden_dim * 2` because bidirectional LSTM has forward + backward outputs.\n",
    "\n",
    "### 4. **Forward Method**\n",
    "```python\n",
    "def forward(self, x):\n",
    "    embedded = self.embedding(x)        # word indices ‚Üí vectors\n",
    "    lstm_out, _ = self.lstm(embedded)   # process sequence\n",
    "    final_hidden = lstm_out[:, -1, :]   # take last timestep\n",
    "    out = self.dropout(final_hidden)    # apply dropout\n",
    "    return self.fc(out)                 # predict class\n",
    "```\n",
    "- Converts input indices ‚Üí embeddings ‚Üí LSTM sequence ‚Üí final hidden state ‚Üí classification output.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bd50ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Embedding layer: looks up the GloVe vector for each word index.\n",
    "# - LSTM layer: processes the sequence forward and backward (bidirectional).\n",
    "# - Fully connected (fc) layer: maps the final hidden state to 2 outputs (binary classification).\n",
    "# - Dropout: prevents overfitting by randomly ‚Äúdropping‚Äù some neurons during training.\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim=128, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix), padding_idx=0\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=300,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=0.5\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        final_hidden = lstm_out[:, -1, :]  # Last timestep\n",
    "        out = self.dropout(final_hidden)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b826dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "bilstm_train = BiLSTMDataset(X_train.tolist(), y_train.tolist(), word_to_idx)\n",
    "bilstm_val = BiLSTMDataset(X_val.tolist(), y_val.tolist(), word_to_idx)\n",
    "\n",
    "train_loader_bilstm = DataLoader(bilstm_train, batch_size=64, shuffle=True)\n",
    "val_loader_bilstm = DataLoader(bilstm_val, batch_size=64)\n",
    "\n",
    "# Model\n",
    "bilstm_model = BiLSTM(embedding_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss()#- Measures how far the model‚Äôs predictions are from the true labels.- Example: if the model predicts [0.8, 0.2] for ‚Äúpositive‚Äù but the true label is ‚Äúnegative,‚Äù the loss will be high.\n",
    "optimizer = torch.optim.Adam(bilstm_model.parameters(), lr=0.001)#- Optimizer tells the model how to fix itself\n",
    "# - Adam is a popular choice that adapts the learning rate for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ce69ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BiLSTM + GloVe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 443/443 [02:42<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.6749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 443/443 [02:13<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 0.5811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 443/443 [01:43<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Loss: 0.3687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 443/443 [01:45<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Loss: 0.2524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 443/443 [01:45<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Loss: 0.2052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 443/443 [01:43<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Loss: 0.1655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 443/443 [01:41<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Loss: 0.1343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 443/443 [01:41<00:00,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Loss: 0.1175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training BiLSTM + GloVe...\")\n",
    "bilstm_model.train()\n",
    "for epoch in range(8):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader_bilstm):\n",
    "        optimizer.zero_grad()\n",
    "        indices = batch['indices'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = bilstm_model(indices)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss/len(train_loader_bilstm):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94bd1984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM model saved!\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "torch.save(bilstm_model.state_dict(), \"../backend/nlp/bilstm_fake_review.pth\")\n",
    "print(\"BiLSTM model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb52e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DistilBERT predictions (from earlier)\n",
    "distilbert_preds = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621e00e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLSTM predictions\n",
    "bilstm_model.eval()\n",
    "bilstm_preds = []\n",
    "with torch.no_grad(): # tells PyTorch not to track gradients (saves memory, faster inference).\n",
    "    for batch in val_loader_bilstm:\n",
    "        indices = batch['indices'].to(device)\n",
    "        outputs = bilstm_model(indices)\n",
    "        pred = torch.argmax(outputs, dim=1).cpu().numpy() # picks the class with the highest score (0 = negative, 1 = positive).\n",
    "        bilstm_preds.extend(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1788834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL COMPARISON ===\n",
      "DistilBERT Accuracy: 0.9736 | F1: 0.9739\n",
      "BiLSTM+GloVe Accuracy: 0.9309 | F1: 0.9303\n"
     ]
    }
   ],
   "source": [
    "print(\"=== MODEL COMPARISON ===\")\n",
    "print(f\"DistilBERT Accuracy: {accuracy_score(true, distilbert_preds):.4f} | F1: {f1_score(true, distilbert_preds):.4f}\")\n",
    "print(f\"BiLSTM+GloVe Accuracy: {accuracy_score(true, bilstm_preds):.4f} | F1: {f1_score(true, bilstm_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2325086a",
   "metadata": {},
   "source": [
    "- It takes predictions from both models for each sample.\n",
    "- Example:\n",
    "- DistilBERT says 0 (negative), BiLSTM says 1 (positive).\n",
    "- Add them: 0 + 1 = 1.\n",
    "- Rule: if sum ‚â• 1 ‚Üí predict 1 (positive). Otherwise ‚Üí 0 (negative).\n",
    "üëâ This is a simple voting ensemble:\n",
    "- If both say 0 ‚Üí final = 0.\n",
    "- If both say 1 ‚Üí final = 1.\n",
    "- If they disagree (0 vs 1) ‚Üí final = 1 (positive wins).\n",
    "\n",
    "üü¶ Step 4: Why Ensemble Helps\n",
    "- Ensemble combines strengths of both models.\n",
    "- DistilBERT is very strong, BiLSTM is slightly weaker but may catch patterns DistilBERT misses.\n",
    "- By voting, you reduce the chance of one model‚Äôs mistake dominating.\n",
    "- Result: Ensemble accuracy (95%) is between the two models, but F1 score (95.5%) is slightly better than BiLSTM alone.\n",
    "\n",
    "‚úÖ In short\n",
    "- DistilBERT and BiLSTM each make predictions.\n",
    "- Ensemble = combine their predictions with a simple rule (majority vote).\n",
    "- This often improves robustness because two models together are less likely to make the same mistake.\n",
    "\n",
    "üí° Think of it like two friends voting on a movie review:\n",
    "- If both say ‚Äúbad,‚Äù it‚Äôs bad.\n",
    "- If both say ‚Äúgood,‚Äù it‚Äôs good.\n",
    "- If one says ‚Äúbad‚Äù and the other says ‚Äúgood,‚Äù the ensemble rule here leans toward ‚Äúgood.‚Äù\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13e12558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENSEMBLE Accuracy: 0.9535 | F1: 0.9550\n"
     ]
    }
   ],
   "source": [
    "# Ensemble (simple voting)\n",
    "ensemble_preds = []\n",
    "for d, b in zip(distilbert_preds, bilstm_preds):\n",
    "    ensemble_preds.append(1 if (d + b) >= 1 else 0)  # majority vote\n",
    "\n",
    "print(f\"ENSEMBLE Accuracy: {accuracy_score(true, ensemble_preds):.4f} | F1: {f1_score(true, ensemble_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b0995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
